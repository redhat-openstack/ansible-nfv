# NFV Performance - Trex

## Description

**NOTE:** Assumes that the instances are using a special image containing Trex and DPDK binaries. An Image can be generated using `playbooks/images/prepare_performance_images.yml`, refer to the playbook's [documentation](https://github.com/redhat-openstack/ansible-nfv/blob/devel/docs/images/prepare_performance_images.md).

Prepares environment for NFV performance scenario using Trex:
* Configure Trex instance

## Automatic Discovery
Automatic discovery is limited to two direct-physical ports attached to trex instance, more than two ports will require the user to pass configuration manually.

This process is based on NFV QE TLV Lab, which consists of several guest instances:
* Trex guest instance:
  * 16 cores (8 Physical + 8 Hyper-Threaded)
  * 8 GB RAM
  * 3 Network interfaces:
    * Management network (normal port type)
    * SR-IOV PF Port 0 (direct-physical port type)
    * SR-IOV PF Port 1 (direct-physical port type)

## Invocation

This playbooks can operate in two modes:

### Create Cloud Resources
Resources are generated by this playbook via `roles/post_install/openstack_tasks` role, refer to the role's [documentation](https://github.com/redhat-openstack/ansible-nfv/blob/devel/docs/tripleo/post_install/openstack_tasks.md).

Set the variable `cloud_resources` to 'create' and supply the required variables mentioned below.

### Pre-existing Cloud Resources

Resources which already exist can be used in this playbook.

Resources are added to dynamic(in-memory) Ansible inventory by this playbook via `roles/post_install/dynamic_host_inventory` role, refer to role's [documentation](https://github.com/redhat-openstack/ansible-nfv/blob/devel/docs/tripleo/post_install/dynamic_host_inventory.md).

Set the variable `cloud_resources` to `external` and supply the required variables mentioned.

## Role Variables

### Resources

Sets the execution mode (resource creation/resource querying).

 **Not** defined by default.
```
cloud_resources: #'create' or 'external'
```

### Flows

Perform trex customization flow (fetch trafficgen repo, customize trex configuration file).

`True` by default
```
trex_instance_config: True
```

### Trex Configuration Variables

 **Note:** Refer to [Trex documentation for an example](https://trex-tgn.cisco.com/trex/doc/trex_manual.html#_basic_configuration).

States if config variables should be parsed during run time, if `False`, a user must pass the required trex configuration variables.

`True` by default.
```
gather_trex_conf_live: True
```

Destination out trex config file which will be created on trex instance.

`/etc/trex_cfg.yaml` by default.
```
trex_conf_file: '/etc/trex_cfg.yaml'
```

List of ports' information to be used in trex conf

 **Not** defined by default, can be discovered automatically or supplied by user.
```
trex_port_info:
  - src_mac: '00:00:00:01:00:00' # Source MAC of first NIC
    dest_mac: '00:00:00:02:00:00' # Destination MAC of second NIC
    ip: '3.3.3.3' # IP Address of first NIC
    default_gw: '4.4.4.4' # Default Gateway (In our scenario IP Address of second NIC)

  - src_mac: '00:00:00:02:00:00' # Source MAC of second NIC
    dest_mac: '00:00:00:01:00:00' # Destination MAC of first NIC
    ip: '4.4.4.4' # IP Address of second NIC
    default_gw: '3.3.3.3' # Default Gateway (In our scenario IP Address of first NIC)
```

Trex's configuration version to be used

`2` by default.
```
trex_cfg_version: 2 # Trex config version
```

Interfaces' PCI slots that will be used by Trex

 **Not** defined by default, can be discovered automatically or supplied by user.
```
trex_interfaces: ['00:03.0', '00:04.0'] # Trex interfaces PCI slots
```

Additional Platform specific configuration/tuning to Trex

Default:
```
trex_platform:
    master_thread_id: 2
    latency_thread_id: 3
    dual_if:
      - socket: 0
        threads: [4,5,6,7,8,9,10,11]
```

### Trafficgen Variables

**NOTE:** This automation is based on [trafficgen]( https://github.com/atheurer/trafficgen).

Clone trafficgen repo.

`True` by default.
```
clone_traffic_gen_repo: True
```

Trafficgen directory to be cloned to.

`/opt/trafficgen/` by default.

```
trafficgen_dir: '/opt/trafficgen/'
```

Traffichen branch to checkout.

`master` by default.
```
trafficgen_branch: 'master'
```

## Example
Examples of running this playbook:

### Create Cloud Resources Example

Sample variable file used `/path/to/openstack_task_vars.yml`(refer to the documentation mentioned [above](#create-cloud-resources) for more info):
```
---

# Tags
setup_os_env: true
user: false
net_port: true
flavor: true
network: true
image: true
instance: true
aggregate: true
resources_output: true
overcloud_name: overcloud

# User creation
users:
  - name: admin
    pass: ptBVnUnHR8AMNsy7E6MmeT6VA
    project: admin
    domain: default
    role: admin
    quota:
      - cores: 40

# Network creation
networks:
  - name: 'external_net_419'
    physical_network: 'dpdk-mgmt'
    segmentation_id: '419'
    allocation_pool_start: '10.35.185.19'
    allocation_pool_end: '10.35.185.28'
    cidr: '10.35.185.30/28'
    enable_dhcp: true
    gateway_ip: '10.35.185.30'
    network_type: vlan
    external: true
    shared: true
    router_name: router

  - name: 'management_net_530'
    allocation_pool_start: '10.10.130.10'
    allocation_pool_end: '10.10.130.200'
    cidr: '10.10.130.0/24'
    enable_dhcp: true
    gateway_ip: '10.10.130.254'
    network_type: vxlan
    external: false
    router_name: router

  - name: 'testpmd_net_nic0_700'
    physical_network: dpdk-data0
    segmentation_id: '700'
    allocation_pool_start: '70.0.0.100'
    allocation_pool_end: '70.0.0.200'
    cidr: '70.0.0.0/24'
    enable_dhcp: false
    gateway_ip: '70.0.0.254'
    network_type: vlan

  - name: 'testpmd_net_nic1_800'
    physical_network: dpdk-data1
    segmentation_id: '800'
    allocation_pool_start: '80.0.0.100'
    allocation_pool_end: '80.0.0.200'
    cidr: '80.0.0.0/24'
    enable_dhcp: false
    gateway_ip: '80.0.0.254'
    network_type: vlan

  - name: 'sriov_net_nic0_700'
    physical_network: sriov-1
    segmentation_id: '700'
    allocation_pool_start: '70.0.0.100'
    allocation_pool_end: '70.0.0.200'
    cidr: '70.0.0.0/24'
    enable_dhcp: false
    gateway_ip: '70.0.0.254'
    network_type: vlan

  - name: 'sriov_net_nic1_800'
    physical_network: sriov-2
    segmentation_id: '800'
    allocation_pool_start: '80.0.0.100'
    allocation_pool_end: '80.0.0.200'
    cidr: '80.0.0.0/24'
    enable_dhcp: false
    gateway_ip: '80.0.0.254'
    network_type: vlan

dns_nameservers:
  - 10.46.0.31
  - 8.8.8.8

# Aggregate group creation
aggregate_groups:
  - name: TREX_AG
    hosts:
      - compute-0.localdomain
    metadata:
      - flavor=trex_ag

  - name: DUT_AG
    hosts:
      - compute-1.localdomain
    metadata:
      - flavor=dut_ag

# Flavor creation
flavors:
  - name: perf_numa_0_trex
    ram: 8192
    disk: 20
    vcpus: 16
    extra_specs:
      - "hw:mem_page_size": "1GB"
        "hw:cpu_policy": "dedicated"
        "hw:numa_nodes": "1"
        "hw:numa_cpus.0": "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15"
        "hw:numa_mem.0": "8192"
        "aggregate_instance_extra_specs:flavor": "trex_ag"

  - name: perf_numa_0_sriov_dut
    ram: 8192
    disk: 20
    vcpus: 16
    extra_specs:
      - "hw:mem_page_size": "1GB"
        "hw:cpu_policy": "dedicated"
        "hw:numa_nodes": "1"
        "hw:numa_cpus.0": "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15"
        "hw:numa_mem.0": "8192"
        "aggregate_instance_extra_specs:flavor": "dut_ag"

  - name: perf_numa_1_dpdk_dut
    ram: 8192
    disk: 20
    vcpus: 8
    extra_specs:
      - "hw:mem_page_size": "1GB"
        "hw:cpu_policy": "dedicated"
        "hw:numa_nodes": "1"
        "hw:numa_cpus.1": "0,1,2,3,4,5,6,7"
        "hw:numa_mem.1": "8192"
        "aggregate_instance_extra_specs:flavor": "dut_ag"

# Image upload to the environment
images:
  - name: trex_testpmd
    url: http://file.tlv.redhat.com/~vkhitrin/rhel-guest-image-7.6-210.x86_64.qcow2

# Keypair creation for the ssh keyless access
keypair_name: test_keypair
resources_output_file: /home/stack/resources_output_file.yml

# Allow ICPM and SSH access to instances
security_groups:
  - name: test_secgroup
    rules:
      - protocol: icmp
        port_range_min: -1
        port_range_max: -1
        remote_ip_prefix: 0.0.0.0/0
      - protocol: tcp
        port_range_min: 22
        port_range_max: 22
        remote_ip_prefix: 0.0.0.0/0

# Instances creation
instances:
  - name: trex
    groups: trex
    flavor:  perf_numa_0_trex
    image: trex_testpmd
    key_name: "{{ keypair_name }}"
    sec_groups: test_secgroup
    config_drive: false
    floating_ip:
      ext_net: external_net_419
      int_net: management_net_530
    nics: net-name=management_net_530,port-name=sriov_net_nic0_700_trex_direct-physical_port-0,port-name=sriov_net_nic1_800_trex_direct-physical_port-1
    net_ports:
      - name: sriov_net_nic0_700_trex_direct-physical_port-0
        network: sriov_net_nic0_700
        port_security: false
        type: direct-physical
      - name: sriov_net_nic1_800_trex_direct-physical_port-1
        network: sriov_net_nic1_800
        port_security: false
        type: direct-physical
```

Generate Trex instance and configure it using information gathered from instance at run time(assuming the instance is confgured as mentioned [above](#automatic-discovery)):
```
ansible-playbook playbooks/packet_gen/trex/performance_scenario.yml -e cloud_resources=create -e @/path/to/openstack_task_vars.yml
```

### Pre-existing Cloud Resources Example

Sample variable file used `/path/to/existing_resources.yml`(refer to the documentation mentioned [above](#pre-existing-cloud-resources) for more info):
```
discover_instance_external_ip: True
dynamic_instances:
  - name: trex
    group: trex
    user: cloud-user
    ssh_key: /tmp/test_keypair.key
```

Configure pre-existing Trex instance using information gathered from instance at run time(assuming the instance is confgured as mentioned [above](#automatic-discovery)):
```
ansible-playbook playbooks/packet_gen/trex/performance_scenario.yml -e cloud_resources=external -e @/path/to/existing_resources.yml
```