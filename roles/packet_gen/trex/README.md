# NFV Performance - Trex

## Description

**NOTE:** Assumes that the instances are using a special image containing Trex and DPDK binaries. An Image can be generated using `playbooks/images/prepare_performance_images.yml`, refer to the playbook's [documentation](/roles/images/performance/README.md).

**NOTE:** Please avoid using instance names for performance scenario that contains underscore(_), the automation will fail otherwise.

Prepares environment for NFV performance scenario using Trex:
* Configure Trex instance
* Binds DPDK NICs
* Launch Trex Server
* Launch TestPMD
* Execute binary_search script to generate traffic

## Automatic Discovery
Automatic discovery is limited to two direct-physical ports attached to trex instance, more than two ports will require the user to pass configuration manually.

This process is based on NFV QE TLV Lab, which consists of several guest instances:
* Trex guest instance:
  * 16 cores (8 Physical + 8 Hyper-Threaded)
  * 8 GB RAM
  * 3 Network interfaces:
    * Management network (normal port type)
    * SR-IOV PF Port 0 (direct-physical port type)
    * SR-IOV PF Port 1 (direct-physical port type)

* DPDK DuT(Device Under Test) guest instance:
  * 8 cores (4 Physical + 4 Hyper-Threaded)
  * 8 GB RAM
  * 3 Network interfaces:
    * Management network (normal port type)
    * DPDK Port 0 (normal port type)
    * DPDK Port 1 (normal port type)

* SR-IOV VF DuT(Device Under Test) guest instance:
  * 8 cores (4 Physical + 4 Hyper-Threaded)
  * 8 GB RAM
  * 3 Network interfaces:
    * Management network (normal port type)
    * SR-IOV PF Port 0 (direct port type)
    * SR-IOV PF Port 1 (direct port type)

## Invocation

This playbook can operate in two modes:

### Create Cloud Resources
Resources are generated by this playbook via `roles/post_install/openstack_tasks` role, refer to the role's [documentation](/roles/post_install/openstack_tasks/README.md).

Set the variable `cloud_resources` to 'create' and supply the required variables mentioned below.

### Pre-existing Cloud Resources

Resources which already exist can be used in this playbook.

Resources are added to dynamic(in-memory) Ansible inventory by this playbook via `roles/post_install/dynamic_host_inventory` role, refer to role's [documentation](/roles/post_install/dynamic_host_inventory/README.md).

Set the variable `cloud_resources` to `external` and supply the required variables mentioned.

## Playbook Variables

### Resources

Sets the execution mode (resource creation/resource querying).

 **Not** defined by default.
```
cloud_resources: #'create' or 'external'
```

### Automation Flows

Perform trex customization flow (fetch trafficgen repo, customize trex configuration file).

`True` by default
```
trex_instance_config: True
```

Binds NICs to DPDK(to user space).

`True` by default
```
bind_dpdk_nics: True
```

Launch Trex Server on trex instance:

`True` by default
```
launch_trex: True
```

Launch TestPMD on DuT instance:

`True` by default
```
launch_testpmd: True
```

Execute binary_search script on trex instance:

`True` by default
```
binary_search: True
```

### Trex Configuration Variables

 **Note:** Refer to [Trex documentation for an example](https://trex-tgn.cisco.com/trex/doc/trex_manual.html#_basic_configuration).

States if config variables should be parsed during run time, if `False`, a user must pass the required trex configuration variables.

`True` by default.
```
gather_trex_conf_live: True
```

Destination out trex config file which will be created on trex instance.

`/etc/trex_cfg.yaml` by default.
```
trex_conf_file: '/etc/trex_cfg.yaml'
```

List of ports' information to be used in trex conf

 **Not** defined by default, can be discovered automatically or supplied by user.
```
trex_port_info:
  - src_mac: '00:00:00:01:00:00' # Source MAC of first NIC
    dest_mac: '00:00:00:02:00:00' # Destination MAC of second NIC
    ip: '3.3.3.3' # IP Address of first NIC
    default_gw: '4.4.4.4' # Default Gateway (In our scenario IP Address of second NIC)

  - src_mac: '00:00:00:02:00:00' # Source MAC of second NIC
    dest_mac: '00:00:00:01:00:00' # Destination MAC of first NIC
    ip: '4.4.4.4' # IP Address of second NIC
    default_gw: '3.3.3.3' # Default Gateway (In our scenario IP Address of first NIC)
```

Trex's configuration version to be used

`2` by default.
```
trex_cfg_version: 2 # Trex config version
```

Interfaces' PCI slots that will be used by Trex

 **Not** defined by default, can be discovered automatically or supplied by user.
```
trex_interfaces: ['00:03.0', '00:04.0'] # Trex interfaces PCI slots
```

Additional Platform specific configuration/tuning to Trex

Default:
```
trex_platform:
    master_thread_id: 2
    latency_thread_id: 3
    dual_if:
      - socket: 0
        threads: [4,5,6,7,8,9,10,11]
```

### Trafficgen Variables

**NOTE:** This automation is based on [trafficgen]( https://github.com/atheurer/trafficgen).

Clone trafficgen repo.

`True` by default.
```
clone_traffic_gen_repo: True
```

Trafficgen URI:

`https://github.com/atheurer/trafficgen` by default.
```
trafficgen_repo: 'https://github.com/atheurer/trafficgen'
```

Trafficgen directory to be cloned to.

`/opt/trafficgen/` by default.

```
trafficgen_dir: '/opt/trafficgen/'
```

Traffichen branch to checkout.

`master` by default.
```
trafficgen_branch: 'master'
```

### DPDK Variables

DPDK root directory on instances(cloned from: 'git://dpdk.org/dpdk').

`/root/dpdk` by default.
```
dpdk_root_dir: '/root/dpdk'
```

Compiled DPDK binaries directory on instances(compiled as part of NFV perf guest image).

Info about what we compile can be found in `prepare_performance_images` [role documentation](/roles/images/performance/README.md#description).

Compilation steps can be found inside `Prepare DPDK binaries inside guest image` [task](/roles/images/performance/tasks/prepare_dpdk.yml).

`{{ dpdk_root_dir }}/build/app` by default.
```
dpdk_compiled_dir: '/root/dpdk/build/app'
```

### Trex Server Variables

Symlinked Trex directory to be used for binary-search(as currently hard coded requirment of binary-search).

`/opt/trex/current` by default.
```
symlinked_trex_dir: '/opt/trex/current'
```

Symlinked t-rex binary to be invoced by binary-search.

`{{ symlinked_trex_dir }}/t-rex-6`  by default.
```
symlinked_trex_bin: '/opt/trex/current/t-rex-64'
```

Number of threads to assign to a trex port pair.

`8`  by default.
```
trex_process_threads: 8
```

Extra command line arugments that can be passed to trex.

Empty by default.
```
trex_process_extra_args: ''
```

### TestPMD Variables

TestPMD binary, assumes DPDK is compiled.

`{{ dpdk_compiled_dir }}/testpmd` by default.
testpmd_bin: "/root/dpdk/testpmd"

Instance cores to act as lcores(map list of instance cores to physical cpu set).
**NOTE:** For each deployment the numbers may vary, if you don't supply correct values, performance will yeild low numbers.

`3,4,7` by default.
```
testpmd_lcores: '3,4,7'
```

Mumber of memory channels to use for TestPMD.

`4` by default.
```
testpmd_mem_channels: 4
```

Memory to allocate(in MBs) per socket for TestPMD.

`1024` by default.
```
testpmd_socket_mem: 1024
```

Number of cores to execute TestPMD on.

`2` by default.
```
testpmd_forward_cores: 2
```

TestPMD TX Queue size.

`1024` by default.
```
testpmd_txd: 1024
```

TestPMD RX Queue size.

`1024` by default.
```
testpmd_rxd: 1024
```

TestPMD forwarding mode, currently supported mode io, mac.

`io` by default.
```
forward_mode: io
```

### Binary Search Sript Variables

binary-search script result log file to be generated on DuT instance.

`/tmp/performance.log` by default.
```
binary_perf_log: '/tmp/performance.log'
```

Trafficgen repo directory on host containg binary-search script.

`/opt/trafficgen` by default.
```
trafficgen_dir: '/opt/trafficgen'
```

binary-search executable on trex instance.

`{{ trafficgen_dir }}/binary-search.py` by default.
```
binary_search_bin: "/opt/trafficgen/binary-search.py"
```

DuT MAC Addresses that trex will send traffic to:

 **Not** defined by default, can be discovered automatically or supplied by user.
```
dut_macs: '00:00:00:03:00:00,00:00:00:04:00:00'
```

binary-search command line:

 **Not** defined by default, can be generated automatically or supplied by user.
```
traffic_cmd: '/opt/trafficgen/binary-search.py --traffic-generator trex-txrx --frame-size 64 --max-loss-pct 0.0 --send-teaching-warmup --dst-macs 00:00:00:03:00:00,00:00:00:04:00:00 --num-flows 1'
```

Frame size in bytes that trex will generate.

`64` by default.
```
trex_frame_size: 64
```

Max packet lost percent(%) acceptable.

`0.00` by default.
```
trex_max_lost_pct: 0.00
```

Number of unique flows.

`1` by default.
trex_flows: 1

Minimum rate for of packets to generate, sized in mpps(milion packets per second).

`2` by default.
```
trex_rate: 2
```
Tiral period in seconds during final validation

`30` by default.
```
trex_validation: 30

```

Tiral period in seconds during binary search

`30` by default.
```
trex_search: 30

```

Enables using vlan-ids with binary search. Vlan id is obtained from vif_details of port associated with Trex VM.

`False` by default.
```
trex_set_vlan: False
```

### Tuning

#### Default emulatorpin
Provides the ability to change the target hosts that the playbook should run on. This variable is required.  
```
dut_compute: compute-1
```

Sets the range of shared cpus that should be set to the emulatorpin.
```
cpu_list: '0,20,1,21'
```


#### EMC insertion probability
Provides the ability to change the target hosts that the playbook should run on. This variable is required.  
```
dut_compute: compute-1
```

Provides the ability to change the Exact Match Cache (EMC) insertion in Open vSwitch

`1` emc insertion enabled by default
```
emc_insert_inv_prob: 1
```
`0` emc insertion disabled
```
emc_insert_inv_prob: 0
```

#### Query multiqueue of OVS-DPDK interfaces
Enables setting number of rxq and txq to use for Testpmd based on multiqueue value of OVS-DPDK physical interfaces.
Reuses the dut_compute variable value from above and is required.
```
dut_compute: compute-1
```
multiqueue_set must be set to true to query OVSDB
```
multiqueue_set: True
```

#### Balanced PMD threads
Enables load balance on PMD threads before or during binary search test.

For OVS controlled load balance:
```
pmd_lb: ovs
```

For user controlled load balance:
```
pmd_lb: user
```

## Example
Examples of running this playbook:

### Create Cloud Resources Example

Sample variable file used `/path/to/openstack_task_vars.yml`(refer to the documentation mentioned [above](#create-cloud-resources) for more info):
```
---

# Tags
setup_os_env: true
user: false
net_port: true
flavor: true
network: true
image: true
instance: true
aggregate: true
resources_output: true
overcloud_name: overcloud

# User creation
users:
  - name: admin
    pass: ptBVnUnHR8AMNsy7E6MmeT6VA
    project: admin
    domain: default
    role: admin
    quota:
      - cores: 40

# Network creation
networks:
  - name: 'external_net_419'
    physical_network: 'dpdk-mgmt'
    segmentation_id: '419'
    allocation_pool_start: '10.35.185.19'
    allocation_pool_end: '10.35.185.28'
    cidr: '10.35.185.30/28'
    enable_dhcp: true
    gateway_ip: '10.35.185.30'
    network_type: vlan
    external: true
    shared: true
    router_name: router

  - name: 'management_net_530'
    allocation_pool_start: '10.10.130.10'
    allocation_pool_end: '10.10.130.200'
    cidr: '10.10.130.0/24'
    enable_dhcp: true
    gateway_ip: '10.10.130.254'
    network_type: vxlan
    external: false
    router_name: router

  - name: 'testpmd_net_nic0_700'
    physical_network: dpdk-data0
    segmentation_id: '700'
    allocation_pool_start: '70.0.0.100'
    allocation_pool_end: '70.0.0.200'
    cidr: '70.0.0.0/24'
    enable_dhcp: false
    gateway_ip: '70.0.0.254'
    network_type: vlan

  - name: 'testpmd_net_nic1_800'
    physical_network: dpdk-data1
    segmentation_id: '800'
    allocation_pool_start: '80.0.0.100'
    allocation_pool_end: '80.0.0.200'
    cidr: '80.0.0.0/24'
    enable_dhcp: false
    gateway_ip: '80.0.0.254'
    network_type: vlan

  - name: 'sriov_net_nic0_700'
    physical_network: sriov-1
    segmentation_id: '700'
    allocation_pool_start: '70.0.0.100'
    allocation_pool_end: '70.0.0.200'
    cidr: '70.0.0.0/24'
    enable_dhcp: false
    gateway_ip: '70.0.0.254'
    network_type: vlan

  - name: 'sriov_net_nic1_800'
    physical_network: sriov-2
    segmentation_id: '800'
    allocation_pool_start: '80.0.0.100'
    allocation_pool_end: '80.0.0.200'
    cidr: '80.0.0.0/24'
    enable_dhcp: false
    gateway_ip: '80.0.0.254'
    network_type: vlan

dns_nameservers:
  - 10.46.0.31
  - 8.8.8.8

# Aggregate group creation
aggregate_groups:
  - name: TREX_AG
    hosts:
      - compute-0.localdomain
    metadata:
      - flavor=trex_ag

  - name: DUT_AG
    hosts:
      - compute-1.localdomain
    metadata:
      - flavor=dut_ag

# Flavor creation
flavors:
  - name: perf_numa_0_trex
    ram: 8192
    disk: 20
    vcpus: 16
    extra_specs:
      - "hw:mem_page_size": "1GB"
        "hw:cpu_policy": "dedicated"
        "hw:numa_nodes": "1"
        "hw:numa_cpus.0": "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15"
        "hw:numa_mem.0": "8192"
        "aggregate_instance_extra_specs:flavor": "trex_ag"

  - name: perf_numa_0_sriov_dut
    ram: 8192
    disk: 20
    vcpus: 16
    extra_specs:
      - "hw:mem_page_size": "1GB"
        "hw:cpu_policy": "dedicated"
        "hw:numa_nodes": "1"
        "hw:numa_cpus.0": "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15"
        "hw:numa_mem.0": "8192"
        "aggregate_instance_extra_specs:flavor": "dut_ag"

  - name: perf_numa_1_dpdk_dut
    ram: 8192
    disk: 20
    vcpus: 8
    extra_specs:
      - "hw:mem_page_size": "1GB"
        "hw:cpu_policy": "dedicated"
        "hw:numa_nodes": "1"
        "hw:numa_cpus.1": "0,1,2,3,4,5,6,7"
        "hw:numa_mem.1": "8192"
        "aggregate_instance_extra_specs:flavor": "dut_ag"

# Image upload to the environment
images:
  - name: trex_testpmd
    url: http://file.tlv.redhat.com/~vkhitrin/rhel-guest-image-7.6-210.x86_64.qcow2

# Keypair creation for the ssh keyless access
keypair_name: test_keypair
resources_output_file: /home/stack/resources_output_file.yml

# Allow ICPM and SSH access to instances
security_groups:
  - name: test_secgroup
    rules:
      - protocol: icmp
        port_range_min: -1
        port_range_max: -1
        remote_ip_prefix: 0.0.0.0/0
      - protocol: tcp
        port_range_min: 22
        port_range_max: 22
        remote_ip_prefix: 0.0.0.0/0

# Instances creation
instances:
  - name: trex
    groups: trex
    flavor:  perf_numa_0_trex
    image: trex_testpmd
    key_name: "{{ keypair_name }}"
    sec_groups: test_secgroup
    config_drive: false
    floating_ip:
      ext_net: external_net_419
      int_net: management_net_530
    nics: net-name=management_net_530,port-name=sriov_net_nic0_700_trex_direct-physical_port-0,port-name=sriov_net_nic1_800_trex_direct-physical_port-1
    net_ports:
      - name: sriov_net_nic0_700_trex_direct-physical_port-0
        network: sriov_net_nic0_700
        port_security: false
        type: direct-physical
      - name: sriov_net_nic1_800_trex_direct-physical_port-1
        network: sriov_net_nic1_800
        port_security: false
        type: direct-physical

  - name: testpmd-sriov-vf-dut
    groups: vm_groups
    flavor: perf_numa_0_sriov_dut
    image: trex_testpmd
    key_name: "{{ keypair_name }}"
    sec_groups: test_secgroup
    config_drive: false
    floating_ip:
      ext_net: external_net_419
      int_net: management_net_530
    nics: net-name=management_net_530,port-name=sriov_net_nic0_700_sriov_vf_dut_direct_port-0,port-name=sriov_net_nic1_800_sriov_vf_dut_direct_port-1
    net_ports:
      - name: sriov_net_nic0_700_sriov_vf_dut_direct_port-0
        network: sriov_net_nic0_700
        port_security: false
        type: direct
      - name: sriov_net_nic1_800_sriov_vf_dut_direct_port-1
        network: sriov_net_nic1_800
        port_security: false
        type: direct

  - name: testpmd-dpdk-dut
    groups: dut
    flavor: perf_numa_1_dpdk_dut
    image: trex_testpmd
    key_name: "{{ keypair_name }}"
    sec_groups: test_secgroup
    config_drive: false
    floating_ip:
      ext_net: external_net_419
      int_net: management_net_530
    nics: net-name=management_net_530,port-name=testpmd_net_nic0_700_dpdk_dut_port-0,port-name=testpmd_net_nic1_800_dpdk_dut_port-1
    net_ports:
      - name: testpmd_net_nic0_700_dpdk_dut_port-0
        network: testpmd_net_nic0_700
        port_security: false
        type: normal
      - name: testpmd_net_nic1_800_dpdk_dut_port-1
        network: testpmd_net_nic1_800
        port_security: false
        type: normal

# DUT compute
dut_compute: compute-1

# Default emulatorpin parameters
cpu_list: '0,20,1,21'

# EMC insertion probability
emc_insert_inv_prob: 0
```

Generate Trex instance and configure it using information gathered from instance at run time(assuming the instance is confgured as mentioned [above](#automatic-discovery)):
```
ansible-playbook playbooks/packet_gen/trex/performance_scenario.yml -e cloud_resources=create -e @/path/to/openstack_task_vars.yml
```

### Pre-existing Cloud Resources Example

Sample variable file used `/path/to/existing_resources.yml`(refer to the documentation mentioned [above](#pre-existing-cloud-resources) for more info):
```
discover_instance_external_ip: True
dut_group: dpdk_dut
dut_type: dpdk
dynamic_instances:
  - name: trex
    group: trex
    user: cloud-user
    ssh_key: /tmp/test_keypair.key
  - name: testpmd-dpdk-dut
    group: dpdk_dut
    user: cloud-user
    ssh_key: /tmp/test_keypair.key
```

Configure pre-existing Trex instance using information gathered from instance at run time(assuming the instance is confgured as mentioned [above](#automatic-discovery)):
```
ansible-playbook playbooks/packet_gen/trex/performance_scenario.yml -e cloud_resources=external -e @/path/to/existing_resources.yml
```
